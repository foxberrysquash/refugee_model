{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a558b-7ff9-46f3-961b-b03af3b79d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Papers:\n",
    "## EvolveGCN\n",
    "#https://jiechenjiechen.github.io/pub/evolvegcn.pdf\n",
    "#Code:https://github.com/IBM/EvolveGCN\n",
    "## Nature paper (Evolve + Attention)\n",
    "#file:///C:/Users/Christian/Downloads/s41598-023-50977-6.pdf\n",
    "## Efficent TGN\n",
    "#https://dl.acm.org/doi/pdf/10.1145/3627673.3679104\n",
    "#PyG\n",
    "#https://pytorch-geometric.readthedocs.io/en/latest/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510558c-23ff-4410-9d12-016f58de942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Home-made functions\n",
    "from utils import year_to_t, unweight_adj, log_transform, exp_transform, get_link\n",
    "from utils.plot import plot_loss, plot_roc\n",
    "\n",
    "from make_data import load_edges, load_features, prepare_language, prepare_data\n",
    "from model import EGCU_H\n",
    "from mlp import LinkPredictorMLP\n",
    "from training import train\n",
    "from eval import eval_baseline, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc708bc-a36d-49ed-abfe-faac3466ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa00d4-27bb-4603-ac2d-7e036e03bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "PATH_NF = \"./data/input/\"\n",
    "PATH_E = \"./data/observation/\"\n",
    "\n",
    "\n",
    "# Data preperations\n",
    "years = np.arange(1992,2019 + 1)\n",
    "time_steps = len(years)\n",
    "countries_codes = pd.read_csv(PATH_NF + \"Country_codes_names.csv\")\n",
    "nodes = len(countries_codes)\n",
    "countries = list(countries_codes[\"Name\"])\n",
    "codes = list(countries_codes[\"Code\"])\n",
    "country_number = countries_codes[\"No\"].to_numpy() - 1\n",
    "languages = list(pd.read_csv(PATH_NF + \"Legend_Language.csv\")[\"Language\"])\n",
    "# node id (number) -> Country name\n",
    "dict_country = dict(zip(country_number,countries))\n",
    "dict_code = dict(zip(country_number,codes))\n",
    "dict_code_to_country = dict(zip(codes,countries))\n",
    "dict_code_to_number = dict(zip(codes,country_number))\n",
    "dict_code_to_language = dict(enumerate(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947234a-2f42-47a1-889c-5f005ca9fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "#PATH_NF = \"/kaggle/input/ds535-node-features-small/\"\n",
    "feature_names = [\"GDP\",\"GINI\",\"HDI\",\"PTS\",\"U5M\"]\n",
    "X_np, non_nan_idx = load_features(PATH_node_features=PATH_NF,years=years,feature_names=feature_names, remove_nan = True)\n",
    "n_to_country_non_nan = np.array(countries)[non_nan_idx]\n",
    "# Load edges\n",
    "#PATH_E = \"/kaggle/input/ds535-refugee/\"\n",
    "A_np = load_edges(PATH_EDGES=PATH_E,log_transfrom_data=True, \n",
    "                  remove_self_loops=False, non_nan_idx=non_nan_idx, mode = \"refugee\", years_range = np.arange(1992,2019 + 1))\n",
    "# Prepare language data\n",
    "LAN_np = prepare_language(PATH=PATH_NF, non_nan_idx=non_nan_idx)\n",
    "A_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6ca42-66fc-47e8-886e-54891c378b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get test data\n",
    "years1 = np.arange(1992,2020 + 1)\n",
    "X_np1, non_nan_idx1 = load_features(PATH_node_features=PATH_NF,years=years1,feature_names=feature_names, remove_nan = True)\n",
    "A_np1 = load_edges(PATH_EDGES=PATH_E,log_transfrom_data=True, \n",
    "                  remove_self_loops=False, non_nan_idx=non_nan_idx, mode = \"refugee\", years_range = years1)\n",
    "# Prepare language data\n",
    "LAN_np1 = prepare_language(PATH=PATH_NF, non_nan_idx=non_nan_idx1)\n",
    "\n",
    "A1 = torch.from_numpy(A_np1)\n",
    "X1 = torch.from_numpy(X_np1)\n",
    "\n",
    "embedding_dim = 4\n",
    "data1 = prepare_data(A1,X1, tt_idx = 28, embedding_dim=embedding_dim, \n",
    "                    LAN_np = LAN_np, weighted = False) # Set LAN_np = None to train without language embeddings\n",
    "A_norm1,X_norm1,A_train1,A_valid,X_test1 = data1\n",
    "A_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca465706-4499-4e4c-9a22-a21a164b0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un-weighted learning (GCN_MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d4754-6fe1-470c-b250-5f652ba5e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters Grid Search\n",
    "\n",
    "output_dim = 16                 # Output dimension of GCN\n",
    "num_gcn_layers = 2              # Number of GCN layers\n",
    "num_heads = 4                   # Number of attention heads\n",
    "loss_weights = [0.9,0.1]\n",
    "\n",
    "A = torch.from_numpy(A_np)\n",
    "X = torch.from_numpy(X_np)\n",
    "\n",
    "tt_idx = 23\n",
    "embedding_dim = 4\n",
    "data = prepare_data(A,X, tt_idx = tt_idx, embedding_dim=embedding_dim,\n",
    "                    LAN_np = LAN_np, weighted = False) # Set LAN_np = None to train without language embeddings\n",
    "A_norm,X_norm,A_train,A_test,X_test = data\n",
    "\n",
    "from itertools import product\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_dim': [32, 64, 128],\n",
    "#    'output_dim': [8, 16, 32],\n",
    "#    'num_gcn_layers': [1, 2, 3],\n",
    "#    'num_heads': [2, 4, 8],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'edge_subset': [100, 200, 300],\n",
    "#    'loss_weights': [[0.9, 0.1], [0.8, 0.2]],\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Placeholder for best parameters and lowest loss\n",
    "best_params = None\n",
    "lowest_loss = float('inf')\n",
    "\n",
    "# Grid search\n",
    "for params in grid:\n",
    "    print(f\"Testing configuration: {params}\")\n",
    "    \n",
    "    # Unpack parameters\n",
    "    hidden_dim = params['hidden_dim']\n",
    "#    output_dim = params['output_dim']\n",
    "#    num_gcn_layers = params['num_gcn_layers']\n",
    "#    num_heads = params['num_heads']\n",
    "    learning_rate = params['learning_rate']\n",
    "    edge_subset = params['edge_subset']\n",
    "#    loss_weights = params['loss_weights']\n",
    "\n",
    "    # Initialize model and optimizer with the current set of hyperparameters\n",
    "    GCN_MA = EGCU_H(\n",
    "        input_dim=X_norm.shape[-1], \n",
    "        hidden_dim=hidden_dim, \n",
    "        output_dim=output_dim, \n",
    "        num_gcn_layers=num_gcn_layers, \n",
    "        num_heads=num_heads, \n",
    "        attention=True\n",
    "    ).to(device)\n",
    "\n",
    "    link_predictor = LinkPredictorMLP(input_dim=output_dim, weighted=False).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        list(GCN_MA.parameters()) + list(link_predictor.parameters()), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    losses = train(\n",
    "        X_norm=X_norm, A_norm=A_norm, A_train=A_train,\n",
    "        model=GCN_MA, link_predictor=link_predictor,\n",
    "        criterion=criterion, optimizer=optimizer, device=device,\n",
    "        num_epochs=50,  # Use fewer epochs during grid search for efficiency\n",
    "        edge_subset=edge_subset, loss_weights=loss_weights,\n",
    "        calc_reverse=True, save_at=10, weighted=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    _, eval_loss = eval_model(\n",
    "        X_norm=X_norm, A_norm=A_norm, A_test=A_test,\n",
    "        model=GCN_MA, link_predictor=link_predictor, \n",
    "        device=device, weighted=False\n",
    "    )\n",
    "    print(f\"Evaluation loss for this configuration: {eval_loss}\")\n",
    "\n",
    "    primary_loss = eval_loss[0]  # Adjust the index based on your tuple structure\n",
    "    print(f\"Primary loss for this configuration: {primary_loss}\")\n",
    "\n",
    "    # Update best parameters if current configuration is better\n",
    "    if primary_loss < lowest_loss:\n",
    "        lowest_loss = primary_loss\n",
    "        best_params = params\n",
    "    \n",
    "print(\"\\nGrid Search Complete\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Lowest loss: {lowest_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fef68a-59a2-4c22-bc11-ea5c1448d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = {'edge_subset' : 200, 'learning_rate' : 0.0001, 'hidden_dim' : 32 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c3094-cbf8-45ad-9c69-3d182607afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build best model\n",
    "\n",
    "A = torch.from_numpy(A_np)\n",
    "X = torch.from_numpy(X_np)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## Train Test Index\n",
    "# Now 80 % is training and 20 % is test\n",
    "tt_idx = 23\n",
    "embedding_dim = 4\n",
    "data = prepare_data(A,X, tt_idx = tt_idx, embedding_dim=embedding_dim,\n",
    "                    LAN_np = LAN_np, weighted = False) # Set LAN_np = None to train without language embeddings\n",
    "A_norm,X_norm,A_train,A_test,X_test = data\n",
    "## Hyperparameters\n",
    "# Training\n",
    "num_epochs = 1000                # Number of training epochs\n",
    "edge_subset = best_params['edge_subset'] # best_params['edge_subset'] = 100    # Number of edges to sample during training\n",
    "calc_reverse = True            # Include reverse order calc of pairs\n",
    "learning_rate = best_params['learning_rate'] # best_params['learning_rate'] = 0.0001 \n",
    "loss_weights = [0.9,0.1]        # Balance positive/negative loss\n",
    "# Model\n",
    "input_dim = X_norm.shape[-1]    # F: feature dimension\n",
    "hidden_dim = best_params['hidden_dim'] # best_params['hidden_dim'] = 32\n",
    "output_dim = 16                 # Output dimension of GCN\n",
    "num_gcn_layers = 2              # Number of GCN layers\n",
    "num_heads = 4                   # Number of attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0ffdf-82d1-4b5c-a250-4670e03b80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "GCN_MA = EGCU_H(input_dim=input_dim, \n",
    "                hidden_dim=hidden_dim, \n",
    "                output_dim=output_dim, \n",
    "                num_gcn_layers=num_gcn_layers,\n",
    "                num_heads = num_heads,\n",
    "                attention = True)\n",
    "\n",
    "# Set tensors to device\n",
    "GCN_MA.to(device)\n",
    "\n",
    "link_predictor = LinkPredictorMLP(input_dim=output_dim, weighted=False).to(device) \n",
    "criterion = nn.BCELoss()  # Cross-entropy loss for link prediction\n",
    "optimizer = optim.Adam(list(GCN_MA.parameters()) + list(link_predictor.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab78652-b94b-4f82-8572-6cd212ac9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "losses = train(X_norm=X_norm,A_norm=A_norm,A_train=A_train,\n",
    "               model=GCN_MA,link_predictor=link_predictor,\n",
    "               criterion=criterion,optimizer=optimizer,device=device,\n",
    "               num_epochs=num_epochs,edge_subset=edge_subset, loss_weights=loss_weights,\n",
    "               calc_reverse=calc_reverse,save_at = 50, weighted = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a31c17-75bb-4d50-a715-a64d5cbca86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "A_pred1, (AUC, AP) = eval_model(X_norm1,A_norm1,A_valid,model=GCN_MA,link_predictor=link_predictor, device=device,weighted=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d7222-1176-449e-8148-3e4d94515a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04446a-b890-42c8-87f3-4b42fd866420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b34c5d-a355-4a05-ad3c-d1fc83176c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding hubs\n",
    "#average metrics across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667e2db-dfcb-406d-a7ff-c21868d1888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Derive filtered_countries from dict_country and non_nan_idx\n",
    "countries = list(dict_country.values())\n",
    "countries = [country.replace(\"United Kingdom of Great Britain and Northern Ireland\", \"United Kingdom\") for country in countries]\n",
    "\n",
    "if isinstance(non_nan_idx, np.ndarray) and non_nan_idx.dtype == np.bool_:\n",
    "    non_nan_idx = np.where(non_nan_idx)[0]\n",
    "filtered_countries = [countries[i] for i in non_nan_idx]\n",
    "\n",
    "\n",
    "def compute_network_flow_metrics_extended(adjacency_matrix, countries):\n",
    "    \"\"\"\n",
    "    Compute extended network flow metrics for a weighted adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "        adjacency_matrix: Weighted adjacency matrix (2D NumPy array).\n",
    "        countries: List of country names corresponding to the nodes.\n",
    "\n",
    "    Returns:\n",
    "        flow_metrics: Dictionary containing inflow, outflow hubs, and flow metrics.\n",
    "        flow_heterogeneity: Global metric representing flow distribution disparity.\n",
    "    \"\"\"\n",
    "    # Compute inflows (sum rows) and outflows (sum columns)\n",
    "    inflows = adjacency_matrix.sum(axis=1)  # Row sums (weighted inflows)\n",
    "    outflows = adjacency_matrix.sum(axis=0)  # Column sums (weighted outflows)\n",
    "\n",
    "    # Compute unweighted inflows and outflows\n",
    "    unweighted_matrix = (adjacency_matrix > 0).astype(int)\n",
    "    inflows_unweighted = unweighted_matrix.sum(axis=1)  # Row sums (unweighted inflows)\n",
    "    outflows_unweighted = unweighted_matrix.sum(axis=0)  # Column sums (unweighted outflows)\n",
    "\n",
    "    # Flow Efficiency (weighted)\n",
    "    flow_efficiency = inflows / (outflows + 1)  # Avoid division by zero\n",
    "\n",
    "    # Net Flow (weighted)\n",
    "    net_flow = inflows - outflows\n",
    "\n",
    "    # Flow Ratio (weighted)\n",
    "    flow_ratio = inflows / (inflows + outflows + 1)  # Avoid division by zero\n",
    "\n",
    "    # Flow Density (weighted)\n",
    "    max_possible_flow = adjacency_matrix.sum()  # Total flow in the network\n",
    "    total_flow = inflows + outflows\n",
    "    flow_density = total_flow / max_possible_flow\n",
    "\n",
    "    # Flow Heterogeneity (Global metric, weighted)\n",
    "    heterogeneity_index = 1 - (np.sum(total_flow**2) / (np.sum(total_flow)**2))\n",
    "\n",
    "    # Normalize inflows and outflows for hub scores (weighted)\n",
    "    max_inflow = inflows.max() if inflows.max() > 0 else 1\n",
    "    max_outflow = outflows.max() if outflows.max() > 0 else 1\n",
    "    normalized_inflows = inflows / max_inflow\n",
    "    normalized_outflows = outflows / max_outflow\n",
    "\n",
    "    # Normalize inflows and outflows for hub scores (unweighted)\n",
    "    max_inflow_unweighted = inflows_unweighted.max() if inflows_unweighted.max() > 0 else 1\n",
    "    max_outflow_unweighted = outflows_unweighted.max() if outflows_unweighted.max() > 0 else 1\n",
    "    normalized_inflows_unweighted = inflows_unweighted / max_inflow_unweighted\n",
    "    normalized_outflows_unweighted = outflows_unweighted / max_outflow_unweighted\n",
    "\n",
    "    # Map results to country names\n",
    "    flow_metrics = {\n",
    "        \"inflow_hubs_weighted\": {countries[i]: normalized_inflows[i] for i in range(len(countries))},\n",
    "        \"outflow_hubs_weighted\": {countries[i]: normalized_outflows[i] for i in range(len(countries))},\n",
    "        \"inflow_hubs_unweighted\": {countries[i]: normalized_inflows_unweighted[i] for i in range(len(countries))},\n",
    "        \"outflow_hubs_unweighted\": {countries[i]: normalized_outflows_unweighted[i] for i in range(len(countries))},\n",
    "        \"flow_efficiency\": {countries[i]: flow_efficiency[i] for i in range(len(countries))},\n",
    "        \"flow_ratio\": {countries[i]: flow_ratio[i] for i in range(len(countries))},\n",
    "        \"flow_density\": {countries[i]: flow_density[i] for i in range(len(countries))},\n",
    "        \"net_flow\": {countries[i]: net_flow[i] for i in range(len(countries))},\n",
    "    }\n",
    "\n",
    "    return flow_metrics, heterogeneity_index\n",
    "\n",
    "# List of metrics to compute\n",
    "metrics_list = [\n",
    "    \"inflow_hubs_weighted\",\n",
    "    \"outflow_hubs_weighted\",\n",
    "    \"inflow_hubs_unweighted\",\n",
    "    \"outflow_hubs_unweighted\",\n",
    "    \"flow_efficiency\",\n",
    "    \"flow_ratio\",\n",
    "    \"flow_density\",\n",
    "    \"net_flow\",\n",
    "]\n",
    "\n",
    "# Initialize data structures to store metrics over time\n",
    "num_years = A_np.shape[0]  # Number of years\n",
    "metrics_over_time = {\n",
    "    metric: {country: [] for country in filtered_countries} for metric in metrics_list\n",
    "}\n",
    "heterogeneity_index_over_time = []\n",
    "\n",
    "# Loop over each year to compute metrics\n",
    "for t in range(num_years):\n",
    "    adjacency_matrix_t = A_np[t, :, :]\n",
    "    adjacency_matrix_t = np.nan_to_num(adjacency_matrix_t)  # Replace NaNs with 0\n",
    "\n",
    "    # Compute metrics for the year\n",
    "    flow_metrics_t, heterogeneity_index_t = compute_network_flow_metrics_extended(\n",
    "        adjacency_matrix_t, filtered_countries\n",
    "    )\n",
    "\n",
    "    # Store metrics for each country\n",
    "    for metric in metrics_list:\n",
    "        for country in filtered_countries:\n",
    "            value = flow_metrics_t[metric][country]\n",
    "            metrics_over_time[metric][country].append(value)\n",
    "\n",
    "    # Store heterogeneity index\n",
    "    heterogeneity_index_over_time.append(heterogeneity_index_t)\n",
    "\n",
    "# Compute average metrics over the years\n",
    "average_metrics = {}\n",
    "for metric in metrics_list:\n",
    "    average_metrics[metric] = {}\n",
    "    for country in filtered_countries:\n",
    "        values = metrics_over_time[metric][country]\n",
    "        average_value = np.mean(values)\n",
    "        average_metrics[metric][country] = average_value\n",
    "\n",
    "# Compute average heterogeneity index\n",
    "average_heterogeneity_index = np.mean(heterogeneity_index_over_time)\n",
    "\n",
    "# Display results for average weighted and unweighted hubs\n",
    "print(\"\\nTop 5 Weighted Inflow Hubs (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"inflow_hubs_weighted\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Weighted Outflow Hubs (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"outflow_hubs_weighted\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Unweighted Inflow Hubs (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"inflow_hubs_unweighted\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Unweighted Outflow Hubs (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"outflow_hubs_unweighted\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Countries by Flow Efficiency (Weighted) (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"flow_efficiency\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Countries by Flow Ratio (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"flow_ratio\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Countries by Flow Density (Average over Years):\")\n",
    "for country, score in sorted(average_metrics[\"flow_density\"].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{country}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Flow Heterogeneity Index (Global): {average_heterogeneity_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1e8f1-7fdc-45a7-b632-b375ff4e7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_top_metrics(flow_metrics, title, metric_key, top_n=5):\n",
    "    \"\"\"\n",
    "    Visualize the top countries for a given metric.\n",
    "    \n",
    "    Args:\n",
    "        flow_metrics: Dictionary containing flow metrics.\n",
    "        title: Title for the plot.\n",
    "        metric_key: Key in the flow_metrics dictionary to visualize.\n",
    "        top_n: Number of top countries to display.\n",
    "    \"\"\"\n",
    "    # Sort and extract top N countries\n",
    "    sorted_metrics = sorted(flow_metrics[metric_key].items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    countries = [item[0] for item in sorted_metrics]\n",
    "    scores = [item[1] for item in sorted_metrics]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.barh(countries, scores, color='skyblue')\n",
    "    plt.xlabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
    "    \n",
    "    # Despine\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_bottom_metrics(flow_metrics, title, metric_key, bottom_n=5):\n",
    "    \"\"\"\n",
    "    Visualize the least-performing countries for a given metric.\n",
    "    \n",
    "    Args:\n",
    "        flow_metrics: Dictionary containing flow metrics.\n",
    "        title: Title for the plot.\n",
    "        metric_key: Key in the flow_metrics dictionary to visualize.\n",
    "        bottom_n: Number of bottom countries to display.\n",
    "    \"\"\"\n",
    "    # Sort and extract bottom N countries\n",
    "    sorted_metrics = sorted(flow_metrics[metric_key].items(), key=lambda x: x[1])[:bottom_n]\n",
    "    countries = [item[0] for item in sorted_metrics]\n",
    "    scores = [item[1] for item in sorted_metrics]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.barh(countries, scores, color='salmon')\n",
    "    plt.xlabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
    "    \n",
    "    # Despine\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize top metrics\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Weighted Inflow Hubs\", \"inflow_hubs_weighted\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Weighted Outflow Hubs\", \"outflow_hubs_weighted\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Unweighted Inflow Hubs\", \"inflow_hubs_unweighted\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Unweighted Outflow Hubs\", \"outflow_hubs_unweighted\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Countries by Flow Efficiency\", \"flow_efficiency\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Countries by Flow Ratio\", \"flow_ratio\")\n",
    "visualize_top_metrics(average_metrics, \"Top 5 Countries by Flow Density\", \"flow_density\")\n",
    "\n",
    "# Visualize bottom metrics\n",
    "visualize_bottom_metrics(average_metrics, \"Bottom 10 Countries by Flow Efficiency\", \"flow_efficiency\", bottom_n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef82079-aaf3-4e91-b792-730141a6988b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a238a-0d42-4efd-a593-10ea8de1b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T -> T+1 (28 -> 29) validation\n",
    "countries = [country.replace(\"United Kingdom of Great Britain and Northern Ireland\", \"United Kingdom\") for country in countries]\n",
    "filtered_countries = [country.replace(\"United Kingdom of Great Britain and Northern Ireland\", \"UK\") for country in filtered_countries]\n",
    "filtered_countries = [country.replace(\"United Kingdom\", \"UK\") for country in filtered_countries]\n",
    "filtered_countries = [country.replace(\"United States of America\", \"USA\") for country in filtered_countries]\n",
    "filtered_countries = [country.replace(\"Syrian Arab Republic\", \"Syria\") for country in filtered_countries]\n",
    "filtered_countries = [country.replace(\"Iran (Islamic Republic of)\", \"Iran\") for country in filtered_countries]\n",
    "filtered_countries = [country.replace(\"Venezuela (Bolivarian Republic of)\", \"Venezuela\") for country in filtered_countries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa7983-2260-4cf7-94a5-448946c09c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_E = \"data/observation/\"\n",
    "years1 = np.arange(1992,2020 + 1)\n",
    "feature_names = [\"GDP\",\"GINI\",\"HDI\",\"PTS\",\"U5M\"]\n",
    "X_np1, non_nan_idx1 = load_features(PATH_node_features=PATH_NF,years=years1,feature_names=feature_names, remove_nan = True)\n",
    "A_np1 = load_edges(PATH_EDGES=PATH_E,log_transfrom_data=False, \n",
    "                  remove_self_loops=False, non_nan_idx=non_nan_idx, mode = \"refugee\", years_range = years1)\n",
    "# Prepare language data\n",
    "LAN_np1 = prepare_language(PATH=PATH_NF, non_nan_idx=non_nan_idx1)\n",
    "\n",
    "A1 = torch.from_numpy(A_np1)\n",
    "X1 = torch.from_numpy(X_np1)\n",
    "\n",
    "data1 = prepare_data(A1,X1, tt_idx = 28, embedding_dim=embedding_dim, \n",
    "                    LAN_np = LAN_np, weighted = False) # Set LAN_np = None to train without language embeddings\n",
    "A_norm1,X_norm1,A_train1,A_valid,X_test1 = data1\n",
    "print(A_norm1.shape)\n",
    "A_pred1, (AUC1, AP1) = eval_model(X_norm1,A_norm1,A_valid,model=GCN_MA,link_predictor=link_predictor, device=device,weighted=False)\n",
    "A_pred_denoised = A_pred1.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b5001-127f-4acf-b60e-6deb948794f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweight_adj(A):\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A_unweighted = (A > 0).astype(int)\n",
    "    elif isinstance(A,torch.Tensor):\n",
    "        A_unweighted = (A > 0).long()\n",
    "    else:\n",
    "        print(\"Error: Array has to be either numpy array or torch tensor\")\n",
    "        A_unweighted = None\n",
    "    return A_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968cb95-da64-4403-a230-f7190049c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gravity model\n",
    "A_np29_gravity29 = pd.read_csv(\"data/Refugee_Stock_2020_Gravity_Model.csv\"\n",
    "                              , index_col=0)\n",
    "A_np29_gravity29 = A_np29_gravity29.fillna(0)\n",
    "\n",
    "max_finite_value = A_np29_gravity29[A_np29_gravity29 != np.inf].max().max()\n",
    "A_np29_gravity29 = A_np29_gravity29.replace([np.inf, -np.inf], max_finite_value)\n",
    "\n",
    "A_np29_gravity29 = np.log(A_np29_gravity29.copy() + 1)\n",
    "\n",
    "A_np29_gravity29.index = A_np29_gravity29.index - 1\n",
    "A_np29_gravity29.columns = A_np29_gravity29.columns.astype(int) - 1\n",
    "\n",
    "A_np29_gravity29 = A_np29_gravity29.iloc[:, non_nan_idx]  # Select columns\n",
    "A_np29_gravity29 = A_np29_gravity29.iloc[non_nan_idx, :]  # Select rows\n",
    "\n",
    "A_np29_gravity29 = A_np29_gravity29.to_numpy()\n",
    "\n",
    "A_np29_gravity29 = unweight_adj(A_np29_gravity29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b6abd-511a-4014-8932-5ed28e69e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_np29 = A_valid.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45b7ee-ece6-4ed0-9611-2b59618d7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Binarize the ground truth graph\n",
    "A_np29_binary = (A_np29 > 0).astype(int)  # Threshold for edge presence\n",
    "\n",
    "# Flatten the matrices\n",
    "y_true = A_np29_binary.flatten()  # Ground truth (binary)\n",
    "y_score = A_np29_gravity29.flatten()  # Predicted values (probabilities)\n",
    "\n",
    "# Compute the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", lw=1)  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for Predicted vs Actual Graph\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f380f-a0af-4bec-9ec7-f207d8631f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8a84f-b0c0-4b21-b313-ed858f5cc112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure A_pred_denoised is a NumPy array\n",
    "A_pred_denoised_np = A_pred_denoised.cpu().detach().numpy()\n",
    "A_valid_np = A_valid.cpu().detach().numpy()  # Convert A_valid to NumPy\n",
    "\n",
    "A_valid_np = A_valid_np.squeeze(0)\n",
    "\n",
    "# Compute metrics for the validation set\n",
    "flow_metrics_valid, flow_heterogeneity_valid = compute_network_flow_metrics_extended(A_valid_np, filtered_countries)\n",
    "\n",
    "# Compute metrics for the predicted graph\n",
    "predicted_flow_metrics, predicted_flow_heterogeneity = compute_network_flow_metrics_extended(A_pred_denoised_np, filtered_countries)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper function: Plot comparison for top 10 countries\n",
    "# -----------------------------\n",
    "def plot_comparison(original_data, predicted_data, title, ylabel):\n",
    "    top_10 = sorted(original_data.items(), key=lambda x: x[1], reverse=True)[:10]  # Top 10 from original\n",
    "    countries, original_values = zip(*top_10)  # Separate into country names and values\n",
    "    predicted_values = [predicted_data[country] for country in countries]  # Get corresponding predicted values\n",
    "    \n",
    "    x = np.arange(len(countries))  # X positions\n",
    "    width = 0.4  # Bar width\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(x - width / 2, original_values, width, label=\"Validation\")\n",
    "    plt.bar(x + width / 2, predicted_values, width, label=\"Predicted\")\n",
    "    plt.xticks(x, countries, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(False)  # Remove grid\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Correlation for each metric and plotting comparisons\n",
    "# -----------------------------\n",
    "\n",
    "metrics_to_compare = {\n",
    "    \"Inflow Hubs\": \"inflow_hubs_weighted\",\n",
    "    \"Outflow Hubs\": \"outflow_hubs_weighted\",\n",
    "}\n",
    "\n",
    "for metric_name, metric_key in metrics_to_compare.items():\n",
    "    print(f\"\\nCorrelation for {metric_name}:\")\n",
    "    corr = np.corrcoef(\n",
    "        list(flow_metrics_valid[metric_key].values()),\n",
    "        list(predicted_flow_metrics[metric_key].values())\n",
    "    )[0, 1]\n",
    "    print(f\"Correlation: {corr:.4f}\")\n",
    "\n",
    "    plot_comparison(\n",
    "        flow_metrics_valid[metric_key],\n",
    "        predicted_flow_metrics[metric_key],\n",
    "        f\"{metric_name} Comparison (Top 10)\",\n",
    "        metric_name\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Flow Heterogeneity Index\n",
    "# -----------------------------\n",
    "print(\"\\nComparison of Flow Heterogeneity Index:\")\n",
    "print(f\"Validation: {flow_heterogeneity_valid:.4f}\")\n",
    "print(f\"Predicted: {predicted_flow_heterogeneity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb5576-fc34-422f-a238-58fa2f887e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure A_pred_denoised is a NumPy array\n",
    "A_pred_denoised_np = A_np29_gravity29\n",
    "A_valid_np = A_valid.cpu().detach().numpy()  # Convert A_valid to NumPy\n",
    "\n",
    "A_valid_np = A_valid_np.squeeze(0)\n",
    "\n",
    "# Compute metrics for the validation set\n",
    "flow_metrics_valid, flow_heterogeneity_valid = compute_network_flow_metrics_extended(A_valid_np, filtered_countries)\n",
    "\n",
    "# Compute metrics for the predicted graph\n",
    "predicted_flow_metrics, predicted_flow_heterogeneity = compute_network_flow_metrics_extended(A_pred_denoised_np, filtered_countries)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper function: Plot comparison for top 10 countries\n",
    "# -----------------------------\n",
    "def plot_comparison(original_data, predicted_data, title, ylabel):\n",
    "    top_10 = sorted(original_data.items(), key=lambda x: x[1], reverse=True)[:10]  # Top 10 from original\n",
    "    countries, original_values = zip(*top_10)  # Separate into country names and values\n",
    "    predicted_values = [predicted_data[country] for country in countries]  # Get corresponding predicted values\n",
    "    \n",
    "    x = np.arange(len(countries))  # X positions\n",
    "    width = 0.4  # Bar width\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(x - width / 2, original_values, width, label=\"Validation\")\n",
    "    plt.bar(x + width / 2, predicted_values, width, label=\"Predicted\")\n",
    "    plt.xticks(x, countries, rotation=45, ha=\"right\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(False)  # Remove grid\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Correlation for each metric and plotting comparisons\n",
    "# -----------------------------\n",
    "\n",
    "metrics_to_compare = {\n",
    "    \"Inflow Hubs\": \"inflow_hubs_weighted\",\n",
    "    \"Outflow Hubs\": \"outflow_hubs_weighted\",\n",
    "}\n",
    "\n",
    "for metric_name, metric_key in metrics_to_compare.items():\n",
    "    print(f\"\\nCorrelation for {metric_name}:\")\n",
    "    corr = np.corrcoef(\n",
    "        list(flow_metrics_valid[metric_key].values()),\n",
    "        list(predicted_flow_metrics[metric_key].values())\n",
    "    )[0, 1]\n",
    "    print(f\"Correlation: {corr:.4f}\")\n",
    "\n",
    "    plot_comparison(\n",
    "        flow_metrics_valid[metric_key],\n",
    "        predicted_flow_metrics[metric_key],\n",
    "        f\"{metric_name} Comparison (Top 10)\",\n",
    "        metric_name\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Flow Heterogeneity Index\n",
    "# -----------------------------\n",
    "print(\"\\nComparison of Flow Heterogeneity Index:\")\n",
    "print(f\"Validation: {flow_heterogeneity_valid:.4f}\")\n",
    "print(f\"Predicted: {predicted_flow_heterogeneity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e176ea-3ffb-41d1-9e16-98a93137754f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c679bc-65fe-49ca-a4f9-9310aa88aeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680107a-2fde-4cfc-a360-7ba538cc1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups for ablation\n",
    "feature_groups = {\n",
    "    \"GDP\": [0],\n",
    "    \"GINI\": [1],\n",
    "    \"HDI\": [2],\n",
    "    \"PTS\": [3],\n",
    "    \"U5M\": [4],\n",
    "    \"Language_Embeddings\": list(range(5, 17))  # Treat all language embeddings as one chunk\n",
    "}\n",
    "\n",
    "# Set number of epochs for retraining\n",
    "num_epochs_retrain = 500\n",
    "\n",
    "# Dictionary to store performance drop for each feature group\n",
    "feature_importance_retrain_500 = {}\n",
    "\n",
    "# Retrain the baseline model (no masking) with 500 epochs\n",
    "print(\"\\nRetraining baseline model with 500 epochs...\")\n",
    "baseline_model_retrained = EGCU_H(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_gcn_layers=num_gcn_layers,\n",
    "    num_heads=num_heads,\n",
    "    attention=True\n",
    ").to(device)\n",
    "\n",
    "link_predictor_baseline_retrained = LinkPredictorMLP(input_dim=output_dim, weighted=False).to(device)\n",
    "optimizer_baseline_retrained = optim.Adam(\n",
    "    list(baseline_model_retrained.parameters()) + list(link_predictor_baseline_retrained.parameters()), \n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# Train baseline retrained model\n",
    "train(\n",
    "    X_norm=X_norm, A_norm=A_norm, A_train=A_train,\n",
    "    model=baseline_model_retrained, link_predictor=link_predictor_baseline_retrained,\n",
    "    criterion=criterion, optimizer=optimizer_baseline_retrained,\n",
    "    device=device, num_epochs=num_epochs_retrain, edge_subset=edge_subset,\n",
    "    loss_weights=loss_weights, calc_reverse=calc_reverse,\n",
    "    save_at=50, weighted=False\n",
    ")\n",
    "\n",
    "# Evaluate the retrained baseline model\n",
    "_, baseline_metrics_retrained = eval_model(\n",
    "    X_norm=X_norm1,\n",
    "    A_norm=A_norm1,\n",
    "    A_test=A_valid,\n",
    "    model=baseline_model_retrained,\n",
    "    link_predictor=link_predictor_baseline_retrained,\n",
    "    device=device,\n",
    "    loss_all=False,\n",
    "    weighted=False\n",
    ")\n",
    "baseline_auc_retrained = baseline_metrics_retrained[1]  # AUC\n",
    "print(f\"Baseline AUC (retrained): {baseline_auc_retrained}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e8cfc-caeb-4b42-95d5-db6fc225ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ablation study with retraining\n",
    "for group_name, indices in feature_groups.items():\n",
    "    print(f\"\\nExcluding {group_name} features and retraining with 500 epochs...\")\n",
    "\n",
    "    # Mask out the features for this group\n",
    "    X_masked = X_norm.clone()\n",
    "    X_masked[..., indices] = 0  # Mask the features for the group\n",
    "\n",
    "    X_masked1 = X_norm1.clone()\n",
    "    X_masked1[..., indices] = 0  # Mask the features for the group\n",
    "\n",
    "    # Retrain the model with the masked features\n",
    "    model_retrain_500 = EGCU_H(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_gcn_layers=num_gcn_layers,\n",
    "        num_heads=num_heads,\n",
    "        attention=True\n",
    "    ).to(device)\n",
    "\n",
    "    link_predictor_retrain_500 = LinkPredictorMLP(input_dim=output_dim, weighted=False).to(device)\n",
    "    optimizer_retrain_500 = optim.Adam(\n",
    "        list(model_retrain_500.parameters()) + list(link_predictor_retrain_500.parameters()), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        X_norm=X_masked, A_norm=A_norm, A_train=A_train,\n",
    "        model=model_retrain_500, link_predictor=link_predictor_retrain_500,\n",
    "        criterion=criterion, optimizer=optimizer_retrain_500,\n",
    "        device=device, num_epochs=num_epochs_retrain, edge_subset=edge_subset,\n",
    "        loss_weights=loss_weights, calc_reverse=calc_reverse,\n",
    "        save_at=50, weighted=False\n",
    "    )\n",
    "\n",
    "    # Evaluate the retrained model\n",
    "    _, retrain_metrics_500 = eval_model(\n",
    "        X_norm=X_masked1,\n",
    "        A_norm=A_norm1,\n",
    "        A_test=A_valid,\n",
    "        model=model_retrain_500,\n",
    "        link_predictor=link_predictor_retrain_500,\n",
    "        device=device,\n",
    "        loss_all=False,\n",
    "        weighted=False\n",
    "    )\n",
    "\n",
    "    retrain_auc_500 = retrain_metrics_500[1]  # AUC\n",
    "    print(f\"AUC after excluding {group_name} (500 epochs): {retrain_auc_500}\")\n",
    "\n",
    "    # Calculate the performance drop\n",
    "    performance_drop = 100 * (baseline_auc_retrained - retrain_auc_500) / baseline_auc_retrained\n",
    "    feature_importance_retrain_500[group_name] = performance_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74b9e3-0b3d-4dbd-8509-009cf11f7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature_importance.keys()\n",
    "key_mapping = {\n",
    "    'Language_Embeddings' : 'Language', \n",
    "    'U5M' : 'Mortality under 5', \n",
    "    'PTS' : 'Political Terror Scale', \n",
    "    'HDI' : 'Human Development Index',\n",
    "    'GINI' : 'GINI index', \n",
    "    'GDP' : 'GDP per capita'\n",
    "}\n",
    "\n",
    "feature_importance_retrain_500 = {\n",
    "    key_mapping.get(key, key): value for key, value in feature_importance_retrain_500.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97f6c6-91c5-4ce4-b68c-ea487c2080e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the order of features and their corresponding performance impacts\n",
    "feature_names = list(feature_importance_retrain_500.keys())\n",
    "performance_increases = list(feature_importance_retrain_500.values())\n",
    "\n",
    "feature_names = feature_names[::-1]\n",
    "performance_increases = performance_increases[::-1]\n",
    "\n",
    "# Create the horizontal bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(feature_names, performance_increases, color='skyblue', alpha=0.8)\n",
    "#plt.ylabel('Feature Groups', fontsize=14)\n",
    "plt.xlabel('Performance Decrease (%)', fontsize=14)\n",
    "plt.title('Feature Ablation Performance Loss', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add a dotted line at the zero point\n",
    "plt.axvline(x=0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "\n",
    "# Remove grid\n",
    "plt.grid(False)\n",
    "\n",
    "# Despine by removing top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff737b-776b-46b2-83f7-02a12c44f2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
